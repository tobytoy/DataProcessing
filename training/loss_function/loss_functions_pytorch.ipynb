{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss functions\n",
    "## 1. L1Loss:\n",
    "``` torch.nn.L1Loss() ```\n",
    "$$\n",
    "\\frac{\\sum_{i=1}^{n} |x_i - y_i|}{n} = \\frac{|x_1 - y_1| + ... + | x_n - y_n |}{n}\n",
    "$$\n",
    "## 2. MSELoss:\n",
    "``` torch.nn.MSELoss( reduction = 'mean' ) ```\n",
    "$$\n",
    "\\frac{\\sum_{i=1}^{n} (x_i - y_i)^2}{n} = \\frac{(x_1 - y_1)^2 + ... + (x_n - y_n)^2}{n}\n",
    "$$\n",
    "## 3. CrossEntropyLoss:\n",
    "* This criterion combines 'log_softmax' and 'nll_loss' in a single function.\n",
    "* The nll_loss function is Negtive Log Likehood function.\n",
    "$$\n",
    "NLLloss = - \\frac{\\sum_{i=1}^{n} y_i logSoftmax(x)_i}{n} \n",
    "$$\n",
    "\n",
    "$$\n",
    "logSoftmax(x) = log( softmax(x) )\n",
    "$$\n",
    "\n",
    "$$\n",
    "softmax(x) = \\{ \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j} } | i = 1...n \\}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: [[ 0.75670105 -0.25799936  0.39240617  0.48379543  0.37870556]]\n",
      "target: [[-0.84618366  0.47963923 -0.23429754  0.12083215 -0.88540864]]\n",
      "diff: [[ 1.6028848  -0.7376386   0.62670374  0.3629633   1.2641141 ]]\n",
      "loss: 0.9188609\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9188609063625336"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# L1Loss\n",
    "import torch\n",
    "\n",
    "n = 5\n",
    "\n",
    "input = torch.randn(1, n, requires_grad=False)\n",
    "target = torch.randn(1, n)\n",
    "\n",
    "loss = torch.nn.L1Loss()\n",
    "\n",
    "print('input:', input.numpy())\n",
    "print('target:', target.numpy())\n",
    "print('diff:', input.numpy() - target.numpy())\n",
    "print('loss:', loss(input, target).numpy())\n",
    "\n",
    "account = 0\n",
    "for element in (input.numpy() - target.numpy())[0]:\n",
    "    account += abs(element)\n",
    "display( account / n )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: [[-2.2133377  -0.00470883  1.595622   -0.4745386   1.5381501 ]]\n",
      "target: [[ 0.09322105 -1.1338818   1.6426686   1.3420199  -0.66468334]]\n",
      "diff: [[-2.3065586   1.1291729  -0.04704666 -1.8165585   2.2028334 ]]\n",
      "loss: 2.9499633\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.949963527871296"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MSELoss\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "n = 5\n",
    "input = torch.randn(1, n, requires_grad=False)\n",
    "target = torch.randn(1, n)\n",
    "\n",
    "loss = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "print('input:', input.numpy())\n",
    "print('target:', target.numpy())\n",
    "print('diff:', input.numpy() - target.numpy())\n",
    "print('loss:', loss(input, target).numpy())\n",
    "\n",
    "account = 0\n",
    "for element in (input.numpy() - target.numpy())[0]:\n",
    "    account += np.square(element)\n",
    "display( account / n )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: tensor([[ 0.1668,  0.8011,  0.7868, -0.1212,  0.9847]])\n",
      "softmax: tensor([[0.1289, 0.2430, 0.2395, 0.0966, 0.2920]])\n",
      "tensor(1.)\n",
      "log softmax: tensor([[-2.0490, -1.4147, -1.4290, -2.3370, -1.2311]])\n"
     ]
    }
   ],
   "source": [
    "# softmax\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "n = 5\n",
    "\n",
    "input = torch.randn(1, n, requires_grad=False)\n",
    "# input = np.random.randn(1, n)\n",
    "input_softmax = torch.exp(input) / torch.sum(torch.exp(input))\n",
    "# input_softmax = np.exp(input) / np.sum(np.exp(input))\n",
    "\n",
    "print('input:', input)\n",
    "print('softmax:', input_softmax)\n",
    "print(torch.sum(input_softmax))\n",
    "\n",
    "\n",
    "# log softmax\n",
    "input_logsoftmax = torch.log(input_softmax)\n",
    "\n",
    "print('log softmax:', input_logsoftmax)\n",
    "\n",
    "\n",
    "\n",
    "# nll loss\n",
    "# https://zhuanlan.zhihu.com/p/159477597\n",
    "\n",
    "\n",
    "\n",
    "# CrossEntropyLoss\n",
    "\n",
    "\n",
    "\n",
    "#input = torch.randn(1, n, requires_grad=False)\n",
    "#target = torch.randn(1, n)\n",
    "\n",
    "#loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#print('input:', input.numpy())\n",
    "#print('target:', target.numpy())\n",
    "\n",
    "\n",
    "#loss(input, target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mweight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mignore_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreduction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSource:\u001b[0m        \n",
      "\u001b[0;32mclass\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_WeightedLoss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34mr\"\"\"This criterion computes the cross entropy loss between input and target.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    It is useful when training a classification problem with `C` classes.\u001b[0m\n",
      "\u001b[0;34m    If provided, the optional argument :attr:`weight` should be a 1D `Tensor`\u001b[0m\n",
      "\u001b[0;34m    assigning weight to each of the classes.\u001b[0m\n",
      "\u001b[0;34m    This is particularly useful when you have an unbalanced training set.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    The `input` is expected to contain raw, unnormalized scores for each class.\u001b[0m\n",
      "\u001b[0;34m    `input` has to be a Tensor of size :math:`(C)` for unbatched input,\u001b[0m\n",
      "\u001b[0;34m    :math:`(minibatch, C)` or :math:`(minibatch, C, d_1, d_2, ..., d_K)` with :math:`K \\geq 1` for the\u001b[0m\n",
      "\u001b[0;34m    `K`-dimensional case. The last being useful for higher dimension inputs, such\u001b[0m\n",
      "\u001b[0;34m    as computing cross entropy loss per-pixel for 2D images.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    The `target` that this criterion expects should contain either:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    - Class indices in the range :math:`[0, C)` where :math:`C` is the number of classes; if\u001b[0m\n",
      "\u001b[0;34m      `ignore_index` is specified, this loss also accepts this class index (this index\u001b[0m\n",
      "\u001b[0;34m      may not necessarily be in the class range). The unreduced (i.e. with :attr:`reduction`\u001b[0m\n",
      "\u001b[0;34m      set to ``'none'``) loss for this case can be described as:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m      .. math::\u001b[0m\n",
      "\u001b[0;34m          \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\u001b[0m\n",
      "\u001b[0;34m          l_n = - w_{y_n} \\log \\frac{\\exp(x_{n,y_n})}{\\sum_{c=1}^C \\exp(x_{n,c})}\u001b[0m\n",
      "\u001b[0;34m          \\cdot \\mathbb{1}\\{y_n \\not= \\text{ignore\\_index}\\}\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m      where :math:`x` is the input, :math:`y` is the target, :math:`w` is the weight,\u001b[0m\n",
      "\u001b[0;34m      :math:`C` is the number of classes, and :math:`N` spans the minibatch dimension as well as\u001b[0m\n",
      "\u001b[0;34m      :math:`d_1, ..., d_k` for the `K`-dimensional case. If\u001b[0m\n",
      "\u001b[0;34m      :attr:`reduction` is not ``'none'`` (default ``'mean'``), then\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m      .. math::\u001b[0m\n",
      "\u001b[0;34m          \\ell(x, y) = \\begin{cases}\u001b[0m\n",
      "\u001b[0;34m              \\sum_{n=1}^N \\frac{1}{\\sum_{n=1}^N w_{y_n} \\cdot \\mathbb{1}\\{y_n \\not= \\text{ignore\\_index}\\}} l_n, &\u001b[0m\n",
      "\u001b[0;34m               \\text{if reduction} = \\text{`mean';}\\\\\u001b[0m\n",
      "\u001b[0;34m                \\sum_{n=1}^N l_n,  &\u001b[0m\n",
      "\u001b[0;34m                \\text{if reduction} = \\text{`sum'.}\u001b[0m\n",
      "\u001b[0;34m            \\end{cases}\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m      Note that this case is equivalent to the combination of :class:`~torch.nn.LogSoftmax` and\u001b[0m\n",
      "\u001b[0;34m      :class:`~torch.nn.NLLLoss`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    - Probabilities for each class; useful when labels beyond a single class per minibatch item\u001b[0m\n",
      "\u001b[0;34m      are required, such as for blended labels, label smoothing, etc. The unreduced (i.e. with\u001b[0m\n",
      "\u001b[0;34m      :attr:`reduction` set to ``'none'``) loss for this case can be described as:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m      .. math::\u001b[0m\n",
      "\u001b[0;34m          \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\u001b[0m\n",
      "\u001b[0;34m          l_n = - \\sum_{c=1}^C w_c \\log \\frac{\\exp(x_{n,c})}{\\sum_{i=1}^C \\exp(x_{n,i})} y_{n,c}\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m      where :math:`x` is the input, :math:`y` is the target, :math:`w` is the weight,\u001b[0m\n",
      "\u001b[0;34m      :math:`C` is the number of classes, and :math:`N` spans the minibatch dimension as well as\u001b[0m\n",
      "\u001b[0;34m      :math:`d_1, ..., d_k` for the `K`-dimensional case. If\u001b[0m\n",
      "\u001b[0;34m      :attr:`reduction` is not ``'none'`` (default ``'mean'``), then\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m      .. math::\u001b[0m\n",
      "\u001b[0;34m          \\ell(x, y) = \\begin{cases}\u001b[0m\n",
      "\u001b[0;34m              \\frac{\\sum_{n=1}^N l_n}{N}, &\u001b[0m\n",
      "\u001b[0;34m               \\text{if reduction} = \\text{`mean';}\\\\\u001b[0m\n",
      "\u001b[0;34m                \\sum_{n=1}^N l_n,  &\u001b[0m\n",
      "\u001b[0;34m                \\text{if reduction} = \\text{`sum'.}\u001b[0m\n",
      "\u001b[0;34m            \\end{cases}\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    .. note::\u001b[0m\n",
      "\u001b[0;34m        The performance of this criterion is generally better when `target` contains class\u001b[0m\n",
      "\u001b[0;34m        indices, as this allows for optimized computation. Consider providing `target` as\u001b[0m\n",
      "\u001b[0;34m        class probabilities only when a single class label per minibatch item is too restrictive.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m        weight (Tensor, optional): a manual rescaling weight given to each class.\u001b[0m\n",
      "\u001b[0;34m            If given, has to be a Tensor of size `C`\u001b[0m\n",
      "\u001b[0;34m        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\u001b[0m\n",
      "\u001b[0;34m            the losses are averaged over each loss element in the batch. Note that for\u001b[0m\n",
      "\u001b[0;34m            some losses, there are multiple elements per sample. If the field :attr:`size_average`\u001b[0m\n",
      "\u001b[0;34m            is set to ``False``, the losses are instead summed for each minibatch. Ignored\u001b[0m\n",
      "\u001b[0;34m            when :attr:`reduce` is ``False``. Default: ``True``\u001b[0m\n",
      "\u001b[0;34m        ignore_index (int, optional): Specifies a target value that is ignored\u001b[0m\n",
      "\u001b[0;34m            and does not contribute to the input gradient. When :attr:`size_average` is\u001b[0m\n",
      "\u001b[0;34m            ``True``, the loss is averaged over non-ignored targets. Note that\u001b[0m\n",
      "\u001b[0;34m            :attr:`ignore_index` is only applicable when the target contains class indices.\u001b[0m\n",
      "\u001b[0;34m        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\u001b[0m\n",
      "\u001b[0;34m            losses are averaged or summed over observations for each minibatch depending\u001b[0m\n",
      "\u001b[0;34m            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\u001b[0m\n",
      "\u001b[0;34m            batch element instead and ignores :attr:`size_average`. Default: ``True``\u001b[0m\n",
      "\u001b[0;34m        reduction (string, optional): Specifies the reduction to apply to the output:\u001b[0m\n",
      "\u001b[0;34m            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will\u001b[0m\n",
      "\u001b[0;34m            be applied, ``'mean'``: the weighted mean of the output is taken,\u001b[0m\n",
      "\u001b[0;34m            ``'sum'``: the output will be summed. Note: :attr:`size_average`\u001b[0m\n",
      "\u001b[0;34m            and :attr:`reduce` are in the process of being deprecated, and in\u001b[0m\n",
      "\u001b[0;34m            the meantime, specifying either of those two args will override\u001b[0m\n",
      "\u001b[0;34m            :attr:`reduction`. Default: ``'mean'``\u001b[0m\n",
      "\u001b[0;34m        label_smoothing (float, optional): A float in [0.0, 1.0]. Specifies the amount\u001b[0m\n",
      "\u001b[0;34m            of smoothing when computing the loss, where 0.0 means no smoothing. The targets\u001b[0m\n",
      "\u001b[0;34m            become a mixture of the original ground truth and a uniform distribution as described in\u001b[0m\n",
      "\u001b[0;34m            `Rethinking the Inception Architecture for Computer Vision <https://arxiv.org/abs/1512.00567>`__. Default: :math:`0.0`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Shape:\u001b[0m\n",
      "\u001b[0;34m        - Input: Shape :math:`(C)`, :math:`(N, C)` or :math:`(N, C, d_1, d_2, ..., d_K)` with :math:`K \\geq 1`\u001b[0m\n",
      "\u001b[0;34m          in the case of `K`-dimensional loss.\u001b[0m\n",
      "\u001b[0;34m        - Target: If containing class indices, shape :math:`()`, :math:`(N)` or :math:`(N, d_1, d_2, ..., d_K)` with\u001b[0m\n",
      "\u001b[0;34m          :math:`K \\geq 1` in the case of K-dimensional loss where each value should be between :math:`[0, C)`.\u001b[0m\n",
      "\u001b[0;34m          If containing class probabilities, same shape as the input and each value should be between :math:`[0, 1]`.\u001b[0m\n",
      "\u001b[0;34m        - Output: If reduction is 'none', same shape as the target. Otherwise, scalar.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        where:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        .. math::\u001b[0m\n",
      "\u001b[0;34m            \\begin{aligned}\u001b[0m\n",
      "\u001b[0;34m                C ={} & \\text{number of classes} \\\\\u001b[0m\n",
      "\u001b[0;34m                N ={} & \\text{batch size} \\\\\u001b[0m\n",
      "\u001b[0;34m            \\end{aligned}\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Examples::\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        >>> # Example of target with class indices\u001b[0m\n",
      "\u001b[0;34m        >>> loss = nn.CrossEntropyLoss()\u001b[0m\n",
      "\u001b[0;34m        >>> input = torch.randn(3, 5, requires_grad=True)\u001b[0m\n",
      "\u001b[0;34m        >>> target = torch.empty(3, dtype=torch.long).random_(5)\u001b[0m\n",
      "\u001b[0;34m        >>> output = loss(input, target)\u001b[0m\n",
      "\u001b[0;34m        >>> output.backward()\u001b[0m\n",
      "\u001b[0;34m        >>>\u001b[0m\n",
      "\u001b[0;34m        >>> # Example of target with class probabilities\u001b[0m\n",
      "\u001b[0;34m        >>> input = torch.randn(3, 5, requires_grad=True)\u001b[0m\n",
      "\u001b[0;34m        >>> target = torch.randn(3, 5).softmax(dim=1)\u001b[0m\n",
      "\u001b[0;34m        >>> output = loss(input, target)\u001b[0m\n",
      "\u001b[0;34m        >>> output.backward()\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0m__constants__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'ignore_index'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reduction'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'label_smoothing'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mignore_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                 \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_smoothing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                               \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                               \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m           ~/opt/anaconda3/envs/yolox/lib/python3.8/site-packages/torch/nn/modules/loss.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     \n"
     ]
    }
   ],
   "source": [
    "??torch.nn.CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dcb9377d4181df79912f2e29f873796f67cdb5163aff27029044c1bb37ff14bf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('yolox')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
